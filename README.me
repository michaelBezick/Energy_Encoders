I made this repository to be able to train a binary autoencoder to both:

  1. achieve adequate reconstruction quality from a 64-bit binary latent space.
  2. have the 64-bit vector be interpreted through an energy function to be
     correlated to the original FOM of the design.

The initial direction was to use QUBO, Blume-Capel, and the Potts model, so I have code for all 3.
QUBO and Blume-Capel was implemented directly from Polytensor, but I used my own implementation of
the Potts model using a Kronecker-delta implementation.

Furthermore, once these models are trained, in the Annealing/ folder is the implementation of classical
annealing with annealing.py. Once the VCA vectors are generated, then the Evaluate_Model/ folder
handles testing the FOM of the novel designs in Calculate_FOM.py.

I initially created the Slurm_Submission/ folder to train non-learnable energy functions on Gilbreth.
Now that we are using a learnable energy function, I create the Slurm_Submission_Learnable/ folder.

For now, it is only really feasible to be training these models on Gilbreth as they take up a lot
of memory.

So in summary:
  Annealing/: folder that has code for VCA to output the best vectors found.
  Evaluate_Model/: folder that is responsible for calculating the FOM of the annealed vectors.
  Files/: has the dataset, labels, and weights for the FOM predictor.
  Models/: Redundant folder
  Modules/: Redundant folder
  Results/: aggregates results from experiments and produces figures
  Slurm_Submission/: for training non-learnable QUBO, Potts, and Blume-Capel on Gilbreth.
  Slurm_Submission_Learnable/: for training learnable PUBO to various degrees.
